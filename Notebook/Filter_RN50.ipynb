{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 3364057,
     "sourceType": "datasetVersion",
     "datasetId": 2029045
    }
   ],
   "dockerImageVersionId": 30176,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install imutils\n",
    "# !pip install -U torchvision\n",
    "# !pip install torchsummary\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-image\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T06:29:45.071996Z",
     "iopub.execute_input": "2024-10-24T06:29:45.072348Z",
     "iopub.status.idle": "2024-10-24T06:29:57.324592Z",
     "shell.execute_reply.started": "2024-10-24T06:29:45.072252Z",
     "shell.execute_reply": "2024-10-24T06:29:57.323547Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T03:27:57.550632Z",
     "start_time": "2024-10-28T03:27:57.547429Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "from math import *\n",
    "import random\n",
    "import xml.etree.ElementTree as ET \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-10-24T06:36:27.529007Z",
     "iopub.execute_input": "2024-10-24T06:36:27.529695Z",
     "iopub.status.idle": "2024-10-24T06:36:27.537020Z",
     "shell.execute_reply.started": "2024-10-24T06:36:27.529657Z",
     "shell.execute_reply": "2024-10-24T06:36:27.536225Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T03:27:58.464796Z",
     "start_time": "2024-10-28T03:27:57.628708Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimutils\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'imutils'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "print(torchvision.__version__)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:34.549567Z",
     "iopub.execute_input": "2024-10-24T01:21:34.549902Z",
     "iopub.status.idle": "2024-10-24T01:21:34.557560Z",
     "shell.execute_reply.started": "2024-10-24T01:21:34.549862Z",
     "shell.execute_reply": "2024-10-24T01:21:34.556799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<span style=\"font-size:25px;\">**Check if GPU is available**</span>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T03:06:06.868218Z",
     "iopub.execute_input": "2024-10-24T03:06:06.868555Z",
     "iopub.status.idle": "2024-10-24T03:06:06.940295Z",
     "shell.execute_reply.started": "2024-10-24T03:06:06.868501Z",
     "shell.execute_reply": "2024-10-24T03:06:06.939490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<span style=\"font-size:25px;\">**Display example data**</span>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<span style=\"font-size:25px;\">**Data preprocessing**</span>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "To prevent the neural network from overfitting the training dataset, the dataset is transoformed randomly. The following operations are applied to the training and validation dataset:\n* Since the face occupies a very small portion of the entire image, crop the image and use only the face for training.\n* Resize the cropped face into a (224x224) image.\n* Randomly change the brightness and saturation of the resized face.\n* Randomly rotate the face after the above three transformations.\n* Convert the image and landmarks into torch tensors and normalize them between [-1, 1].",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def crop_face(self, image, landmarks, crops):\n",
    "        top = int(crops['top'])\n",
    "        left = int(crops['left'])\n",
    "        height = int(crops['height'])\n",
    "        width = int(crops['width'])\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "        return image, landmarks\n",
    "    \n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "    \n",
    "    def color_jitter(self, image, landmarks):\n",
    "        #ranNum = random.random()\n",
    "        color_jitter = transforms.ColorJitter(brightness=random.random(), \n",
    "                                              contrast=random.random(),\n",
    "                                              saturation=random.random(), \n",
    "                                              hue=random.uniform(0,0.5))\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks\n",
    "    \n",
    "    def rotate(self, image, landmarks, angle):\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        landmarks = landmarks - 0.5\n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        new_landmarks = new_landmarks + 0.5\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "    \n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops)\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        image, landmarks = self.rotate(image, landmarks, angle=random.randint(-30,30))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image, landmarks"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T06:36:39.081990Z",
     "iopub.execute_input": "2024-10-24T06:36:39.082289Z",
     "iopub.status.idle": "2024-10-24T06:36:39.098251Z",
     "shell.execute_reply.started": "2024-10-24T06:36:39.082253Z",
     "shell.execute_reply": "2024-10-24T06:36:39.097389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": "<span style=\"font-size:25px;\">**Dataset Preparation**</span>",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "The labels_ibug_300W_train.xml contains the image path, landmarks and coordinates for the bounding box. Store these values in lists to access them easily during training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        tree = ET.parse('../data/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "        root = tree.getroot()\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.root_dir = '../data/ibug_300W_large_face_landmark_dataset'\n",
    "        \n",
    "        for filename in root[2]:\n",
    "            self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "\n",
    "            self.crops.append(filename[0].attrib)\n",
    "\n",
    "            landmark = []\n",
    "            for num in range(68):\n",
    "                x_coordinate = int(filename[0][num].attrib['x'])\n",
    "                y_coordinate = int(filename[0][num].attrib['y'])\n",
    "                landmark.append([x_coordinate, y_coordinate])\n",
    "            self.landmarks.append(landmark)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks).astype('float32')     \n",
    "\n",
    "        assert len(self.image_filenames) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_filenames[index], 0)\n",
    "        landmarks = self.landmarks[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image, landmarks = self.transform(image, landmarks, self.crops[index])\n",
    "\n",
    "        landmarks = landmarks - 0.5\n",
    "\n",
    "        return image, landmarks"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T06:36:42.479617Z",
     "iopub.execute_input": "2024-10-24T06:36:42.480439Z",
     "iopub.status.idle": "2024-10-24T06:36:46.399893Z",
     "shell.execute_reply.started": "2024-10-24T06:36:42.480400Z",
     "shell.execute_reply": "2024-10-24T06:36:46.399170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "OriDataset = FaceLandmarksDataset()\n",
    "image, landmarks = OriDataset[150]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image, cmap='gray');\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s=40);"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T06:39:48.118291Z",
     "iopub.execute_input": "2024-10-24T06:39:48.118600Z",
     "iopub.status.idle": "2024-10-24T06:39:48.518595Z",
     "shell.execute_reply.started": "2024-10-24T06:39:48.118564Z",
     "shell.execute_reply": "2024-10-24T06:39:48.517881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<span style=\"font-size:25px;\">**Example of image after pre-processing**</span>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "dataset = FaceLandmarksDataset(Transforms())\n\nimage, landmarks = dataset[5]\nlandmarks = (landmarks + 0.5) * 224\nplt.figure(figsize=(10, 10))\nplt.imshow(image.numpy().squeeze(), cmap='gray');\nplt.scatter(landmarks[:,0], landmarks[:,1], s=40);",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T06:40:59.173931Z",
     "iopub.execute_input": "2024-10-24T06:40:59.174252Z",
     "iopub.status.idle": "2024-10-24T06:41:03.315531Z",
     "shell.execute_reply.started": "2024-10-24T06:40:59.174215Z",
     "shell.execute_reply": "2024-10-24T06:41:03.314798Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T04:09:25.780025Z",
     "start_time": "2024-10-28T04:09:25.659144Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FaceLandmarksDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mFaceLandmarksDataset\u001B[49m(Transforms())\n\u001B[1;32m      3\u001B[0m image, landmarks \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;241m5\u001B[39m]\n\u001B[1;32m      4\u001B[0m landmarks \u001B[38;5;241m=\u001B[39m (landmarks \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.5\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m224\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'FaceLandmarksDataset' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Train-Test split\n",
    "len_test_set = int(0.2*len(dataset))\n",
    "len_train_set = len(dataset) - len_test_set \n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Test set is {}\".format(len_test_set))\n",
    "\n",
    "train_dataset , test_dataset = torch.utils.data.random_split(dataset , [len_train_set, len_test_set])\n",
    "\n",
    "# setting batch sizes and shuffle the data\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=6)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=6)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:43.734930Z",
     "iopub.execute_input": "2024-10-24T01:21:43.735122Z",
     "iopub.status.idle": "2024-10-24T01:21:43.744161Z",
     "shell.execute_reply.started": "2024-10-24T01:21:43.735097Z",
     "shell.execute_reply": "2024-10-24T01:21:43.743326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "images, landmarks = next(iter(train_loader))\n#torch.set_printoptions(profile=\"full\")\n#print(images.shape)\n#print(landmarks.shape)\nprint(f\"Images batch shape: {images.size()}\")\nprint(f\"Landmarks batch shape: {landmarks.size()}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:43.745226Z",
     "iopub.execute_input": "2024-10-24T01:21:43.745454Z",
     "iopub.status.idle": "2024-10-24T01:21:45.754105Z",
     "shell.execute_reply.started": "2024-10-24T01:21:43.745391Z",
     "shell.execute_reply": "2024-10-24T01:21:45.753223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "images, landmarks = next(iter(test_loader))\nprint(f\"Images batch shape: {images.size()}\")\nprint(f\"Landmarks batch shape: {landmarks.size()}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:45.755802Z",
     "iopub.execute_input": "2024-10-24T01:21:45.756570Z",
     "iopub.status.idle": "2024-10-24T01:21:47.666346Z",
     "shell.execute_reply.started": "2024-10-24T01:21:45.756524Z",
     "shell.execute_reply": "2024-10-24T01:21:47.665468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class ResNet50(nn.Module):\n    def __init__(self,num_classes=136):\n        super().__init__()\n        self.model_name='resnet50'\n        self.model=models.resnet50()\n        self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x=self.model(x)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.667845Z",
     "iopub.execute_input": "2024-10-24T01:21:47.668083Z",
     "iopub.status.idle": "2024-10-24T01:21:47.675207Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.668054Z",
     "shell.execute_reply": "2024-10-24T01:21:47.674317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class ResNet101(nn.Module):\n    def __init__(self,num_classes=136):\n        super().__init__()\n        self.model_name='resnet101'\n        self.model=models.resnet101()\n        self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x=self.model(x)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.678924Z",
     "iopub.execute_input": "2024-10-24T01:21:47.679160Z",
     "iopub.status.idle": "2024-10-24T01:21:47.686048Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.679127Z",
     "shell.execute_reply": "2024-10-24T01:21:47.685315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class Inception(nn.Module):\n    def __init__(self,num_classes=136):\n        super().__init__()\n        self.model_name='inceptionv3'\n        self.model=models.inception_v3()\n        self.model.Conv2d_1a_3x3.conv=nn.Conv2d(1, 32, kernel_size=3, stride=2, bias=False)\n        self.model.AuxLogits.fc = nn.Linear(self.model.AuxLogits.fc.in_features, num_classes)\n        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x=self.model(x)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.687128Z",
     "iopub.execute_input": "2024-10-24T01:21:47.687335Z",
     "iopub.status.idle": "2024-10-24T01:21:47.694591Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.687308Z",
     "shell.execute_reply": "2024-10-24T01:21:47.693757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class DenseNet(nn.Module):\n    def __init__(self,num_classes=136):\n        super().__init__()\n        self.model_name='densenet'\n        self.model = models.densenet161()\n        self.model.features.conv0=nn.Conv2d(1, 96, kernel_size=7, stride=2, padding=3, bias=False)\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, num_classes)\n        \n    def forward(self, x):\n        x=self.model(x)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.695528Z",
     "iopub.execute_input": "2024-10-24T01:21:47.695724Z",
     "iopub.status.idle": "2024-10-24T01:21:47.705386Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.695699Z",
     "shell.execute_reply": "2024-10-24T01:21:47.704740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self,num_classes=136):\n",
    "        super().__init__()\n",
    "        self.model_name='efficientnet'\n",
    "        self.model = models.efficientnet_b4()\n",
    "        self.model.features[0][0]=nn.Conv2d(1, 48, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.model.classifier[1]= nn.Linear(self.model.classifier[1].in_features, num_classes)\n",
    "        \n",
    "        # Khởi tạo lại running_mean và running_var của lớp BatchNorm2d\n",
    "        self.model.features[0][1].reset_running_stats() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.706282Z",
     "iopub.execute_input": "2024-10-24T01:21:47.706528Z",
     "iopub.status.idle": "2024-10-24T01:21:47.714173Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.706500Z",
     "shell.execute_reply": "2024-10-24T01:21:47.713319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class ConvNeXt(nn.Module):\n    def __init__(self,num_classes=136):\n        super().__init__()\n        self.model_name='convnext'\n        self.model = models.convnext_base()\n        self.model.features[0][0]=nn.Conv2d(1, 128, kernel_size=4, stride=4, bias=False)\n        self.model.classifier[2]= nn.Linear(self.model.classifier[2].in_features, num_classes)\n        \n    def forward(self, x):\n        x=self.model(x)\n        return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.715238Z",
     "iopub.execute_input": "2024-10-24T01:21:47.715416Z",
     "iopub.status.idle": "2024-10-24T01:21:47.721985Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.715394Z",
     "shell.execute_reply": "2024-10-24T01:21:47.721228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import sys\n\ndef print_overwrite(step, total_step, loss, operation):\n    sys.stdout.write('\\r')\n    if operation == 'train':\n        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))   \n    else:\n        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))\n        \n    sys.stdout.flush()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:21:47.723148Z",
     "iopub.execute_input": "2024-10-24T01:21:47.723469Z",
     "iopub.status.idle": "2024-10-24T01:21:47.730338Z",
     "shell.execute_reply.started": "2024-10-24T01:21:47.723432Z",
     "shell.execute_reply": "2024-10-24T01:21:47.729754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "##### if os.path.isdir('progress'):\n    !rm -rf progress\nos.mkdir('progress')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-04T20:00:38.427181Z",
     "iopub.execute_input": "2022-05-04T20:00:38.427657Z",
     "iopub.status.idle": "2022-05-04T20:00:38.433096Z",
     "shell.execute_reply.started": "2022-05-04T20:00:38.427618Z",
     "shell.execute_reply": "2022-05-04T20:00:38.432129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model1 = ResNet50()\n",
    "model2 = ResNet101()\n",
    "# model3 = Inception()\n",
    "# model4 = DenseNet()\n",
    "# model5 = EfficientNet()\n",
    "model6 = ConvNeXt()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:22:41.922159Z",
     "iopub.execute_input": "2024-10-24T01:22:41.922976Z",
     "iopub.status.idle": "2024-10-24T01:22:44.280630Z",
     "shell.execute_reply.started": "2024-10-24T01:22:41.922933Z",
     "shell.execute_reply": "2024-10-24T01:22:44.279975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def euclidean_distance(predictions, landmarks):\n",
    "    \"\"\"\n",
    "    Tính khoảng cách Euclidean giữa các landmark dự đoán và thực tế.\n",
    "\n",
    "    Args:\n",
    "      predictions: Các landmark dự đoán, có dạng (batch_size, 68, 2).\n",
    "      landmarks: Các landmark thực tế, có dạng (batch_size, 68, 2).\n",
    "\n",
    "    Returns:\n",
    "      Khoảng cách Euclidean trung bình giữa các landmark.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.sqrt(torch.sum((predictions - landmarks) ** 2, dim=2)))\n",
    "\n",
    "def train_test(model, epochs, learning_rate, train_loader, test_loader):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    network = model\n",
    "    # network.load_state_dict(\"checkpoint/progress2.pth\")\n",
    "    network.load_state_dict(torch.load(\"checkpoint/progress.pth\"))  # Đúng\n",
    "    network.to(device)    \n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    num_epochs = epochs\n",
    "    loss_min = np.inf\n",
    "    \n",
    "    train_loss_record = []\n",
    "    test_loss_record = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "\n",
    "        loss_train = 0\n",
    "        loss_test = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        network.train()\n",
    "        for step in range(1,len(train_loader)+1):\n",
    "\n",
    "            images, landmarks = next(iter(train_loader))\n",
    "\n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).to(device) \n",
    "\n",
    "            predictions = network(images)\n",
    "\n",
    "            # clear all the gradients before calculating them\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_train_step = criterion(predictions,landmarks)\n",
    "\n",
    "            #loss_valid_step = criterion(predictions.logits, landmarks)\n",
    "\n",
    "            # calculate the gradients\n",
    "            loss_train_step.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train = loss_train + loss_train_step.item()\n",
    "            running_loss = loss_train/step\n",
    "\n",
    "            print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "            \n",
    "            # Tính toán độ chính xác trên tập huấn luyện\n",
    "            predictions = (predictions.view(-1, 68, 2).cpu() + 0.5) * 224\n",
    "            landmarks = (landmarks.view(-1, 68, 2).cpu() + 0.5) * 224\n",
    "            train_accuracy += euclidean_distance(predictions, landmarks)\n",
    "\n",
    "        network.eval() \n",
    "        with torch.no_grad():\n",
    "\n",
    "            for step in range(1,len(test_loader)+1):\n",
    "\n",
    "                images, landmarks = next(iter(test_loader))\n",
    "\n",
    "                images = images.to(device)\n",
    "                landmarks = landmarks.view(landmarks.size(0),-1).to(device)\n",
    "\n",
    "                predictions = network(images)\n",
    "\n",
    "                # find the loss for the current step\n",
    "                loss_test_step = criterion(predictions, landmarks)\n",
    "                #loss_valid_step = criterion(predictions.logits, landmarks)\n",
    "\n",
    "                loss_test = loss_test + loss_test_step.item()\n",
    "                running_loss = loss_test/step\n",
    "\n",
    "                print_overwrite(step, len(test_loader), running_loss, 'test')\n",
    "                \n",
    "                # Tính toán độ chính xác trên tập kiểm tra\n",
    "                predictions = (predictions.view(-1, 68, 2).cpu() + 0.5) * 224\n",
    "                landmarks = (landmarks.view(-1, 68, 2).cpu() + 0.5) * 224\n",
    "                test_accuracy += euclidean_distance(predictions, landmarks)\n",
    "\n",
    "        loss_train = loss_train / len(train_loader)\n",
    "        loss_test = loss_test / len(test_loader)\n",
    "        \n",
    "        train_accuracy = train_accuracy / len(train_loader)\n",
    "        test_accuracy = test_accuracy / len(test_loader)\n",
    "\n",
    "        print('\\n--------------------------------------------------')\n",
    "        print('Epoch: {}  Train Loss: {:.6f}  Test Loss: {:.6f}'.format(epoch, loss_train, loss_test))\n",
    "        print('Train Accuracy: {:.6f}  Test Accuracy: {:.6f}'.format(train_accuracy, test_accuracy))\n",
    "        print('--------------------------------------------------')\n",
    "        \n",
    "        train_loss_record.append(loss_train)\n",
    "        test_loss_record.append(loss_test)\n",
    "\n",
    "        if loss_test < loss_min:\n",
    "            loss_min = loss_test\n",
    "            torch.save(network.state_dict(), 'checkpoint/progress.pth')  \n",
    "            print(\"\\nMinimum Test Loss of {:.6f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "            print('Model Saved\\n')\n",
    "\n",
    "    print('Training Complete')\n",
    "    print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))\n",
    "\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.plot(train_loss_record)\n",
    "    plt.xticks(range(1,len(train_loss_record)+1, 1))\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Test Loss\")\n",
    "    plt.plot(test_loss_record)\n",
    "    plt.xticks(range(1,len(test_loss_record)+1, 1))\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.xlabel('Epochs')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:23:07.360622Z",
     "iopub.execute_input": "2024-10-24T01:23:07.360909Z",
     "iopub.status.idle": "2024-10-24T01:23:07.380161Z",
     "shell.execute_reply.started": "2024-10-24T01:23:07.360879Z",
     "shell.execute_reply": "2024-10-24T01:23:07.379494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def show_result(model, test_loader):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        best_network = model\n",
    "        best_network.to(device)\n",
    "        best_network.load_state_dict(torch.load('/home/phong/PycharmProjects/Filter project/notebook/checkpoint/progress2.pth')) \n",
    "        best_network.eval()\n",
    " \n",
    "        images, landmarks = next(iter(test_loader))\n",
    "\n",
    "        images = images.to(device)\n",
    "        landmarks = (landmarks + 0.5) * 224   \n",
    "\n",
    "        predictions = (best_network(images).cpu() + 0.5) * 224\n",
    "        predictions = predictions.view(-1,68,2)\n",
    "\n",
    "        plt.figure(figsize=(10,40))\n",
    "\n",
    "        for img_num in range(10):\n",
    "            plt.subplot(1,10,img_num+1)\n",
    "            plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray')\n",
    "            plt.scatter(predictions[img_num,:,0], predictions[img_num,:,1], c = 'r', s = 5)\n",
    "            plt.scatter(landmarks[img_num,:,0], landmarks[img_num,:,1], c = 'g', s = 5)\n",
    "\n",
    "    print('Total number of test images: {}'.format(len(test_dataset)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Elapsed Time : {}\".format(end_time - start_time)) "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:23:12.258027Z",
     "iopub.execute_input": "2024-10-24T01:23:12.258814Z",
     "iopub.status.idle": "2024-10-24T01:23:12.269299Z",
     "shell.execute_reply.started": "2024-10-24T01:23:12.258755Z",
     "shell.execute_reply": "2024-10-24T01:23:12.268611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-24T01:33:27.677976Z",
     "iopub.execute_input": "2024-10-24T01:33:27.678736Z",
     "iopub.status.idle": "2024-10-24T01:44:15.642223Z",
     "shell.execute_reply.started": "2024-10-24T01:33:27.678683Z",
     "shell.execute_reply": "2024-10-24T01:44:15.641283Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": "# train_test(model1,60, 0.0001,train_loader,test_loader)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# show_result(model1, test_loader)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import datetime\n",
    "# \n",
    "# best_network = ResNet50()\n",
    "# best_network.load_state_dict(torch.load('checkpoint/progress.pth'))\n",
    "# best_network = best_network.to(device)  # Chuyển mô hình lên GPU\n",
    "# best_network.eval()\n",
    "# \n",
    "# # Đọc ảnh mới\n",
    "# image_path = 'test9.jpg'\n",
    "# image = cv2.imread(image_path, 1)  # Đọc ảnh xám\n",
    "# plt.imshow(image)\n",
    "# # Phát hiện khuôn mặt (ví dụ sử dụng OpenCV)\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# faces = face_cascade.detectMultiScale(image,1.1, 4)\n",
    "# \n",
    "# # Tiền xử lý ảnh\n",
    "# for (x, y, w, h) in faces:\n",
    "#     face_image_gray = cv2.cvtColor(image[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)  # Chuyển sang ảnh xám\n",
    "#     face_image = Image.fromarray(face_image_gray)\n",
    "#     face_image = transforms.Resize((224, 224))(face_image)\n",
    "#     face_image = transforms.ToTensor()(face_image)\n",
    "#     face_image = transforms.Normalize([0.5], [0.5])(face_image)\n",
    "#     face_image = face_image.unsqueeze(0).to(device)  # Thêm chiều batch\n",
    "# \n",
    "#     # Dự đoán landmark\n",
    "#     with torch.no_grad():\n",
    "#         predictions = best_network(face_image)\n",
    "#     predictions = (predictions.cpu() + 0.5) * 224\n",
    "#     predictions = predictions.view(-1, 68, 2).numpy()\n",
    "#     \n",
    "#     scale_x = w / 224  # w là chiều rộng của bounding box\n",
    "#     scale_y = h / 224  # h là chiều cao của bounding box\n",
    "#         \n",
    "#         # Chuyển đổi tọa độ landmark về ảnh gốc\n",
    "#     for i in range(68):\n",
    "#         landmark_x = int(predictions[0, i, 0] * scale_x + x)  # x là tọa độ x của bounding box\n",
    "#         landmark_y = int(predictions[0, i, 1] * scale_y + y)  # y là tọa độ y của bounding box\n",
    "#     \n",
    "#         # Vẽ landmark lên ảnh gốc\n",
    "#         cv2.circle(image, (landmark_x, landmark_y), 2, (0, 255, 0), -1)\n",
    "#     # Vẽ landmark lên ảnh gốc\n",
    "#     # for i in range(68):\n",
    "#     #     cv2.circle(image, (int(x + predictions[0, i, 0]), int(y + predictions[0, i, 1])), 2, (0, 255, 0), -1)\n",
    "#     cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)    \n",
    "# \n",
    "# current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# \n",
    "# # Tạo thư mục results nếu chưa tồn tại\n",
    "# results_dir = 'Results'\n",
    "# if not os.path.exists(results_dir):\n",
    "#     os.makedirs(results_dir)\n",
    "# \n",
    "# # Lưu kết quả với tên file là ngày giờ\n",
    "# output_filename = f\"{current_datetime}.jpg\"\n",
    "# output_path = os.path.join(results_dir, output_filename)\n",
    "# cv2.imwrite(output_path, image)\n",
    "# # Hiển thị kết quả\n",
    "# cv2.imshow('Result', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import cv2\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# import numpy as np\n",
    "# \n",
    "# # ... (Các hàm và lớp đã được định nghĩa trước đó, bao gồm ResNet50, Transforms, ...) ...\n",
    "# \n",
    "# # Thiết bị (GPU nếu có)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# \n",
    "# # Nạp mô hình đã huấn luyện\n",
    "# best_network = ResNet50()  # Hoặc kiến trúc mạng tương ứng\n",
    "# best_network.load_state_dict(torch.load('checkpoint/trainRS50_120.pth'))\n",
    "# best_network.to(device)\n",
    "# best_network.eval()\n",
    "# \n",
    "# # Khởi tạo camera\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# \n",
    "# # Nạp mô hình phát hiện khuôn mặt Haar Cascade\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# \n",
    "# while(True):\n",
    "#     # Đọc một khung hình từ camera\n",
    "#     ret, frame = cap.read()\n",
    "# \n",
    "#     # Phát hiện khuôn mặt\n",
    "#     faces = face_cascade.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=8)\n",
    "# \n",
    "#     # Tiền xử lý ảnh và dự đoán landmark cho từng khuôn mặt\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         face_image_gray = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)  # Chuyển sang ảnh xám\n",
    "#         face_image = Image.fromarray(face_image_gray)\n",
    "#         face_image = transforms.Resize((224, 224))(face_image)\n",
    "#         face_image = transforms.ToTensor()(face_image)\n",
    "#         face_image = transforms.Normalize([0.5], [0.5])(face_image)\n",
    "#         face_image = face_image.unsqueeze(0).to(device)  # Thêm chiều batch\n",
    "# \n",
    "#         # Dự đoán landmark\n",
    "#         with torch.no_grad():\n",
    "#             predictions = best_network(face_image)\n",
    "#         predictions = (predictions.cpu() + 0.5) * 224\n",
    "#         predictions = predictions.view(-1, 68, 2).numpy()\n",
    "# \n",
    "#         # Vẽ landmark lên ảnh gốc (ảnh màu)\n",
    "#         # for i in range(68):\n",
    "#         #     cv2.circle(frame, (int(x + predictions[0, i, 0]), int(y + predictions[0, i, 1])), 2, (0, 255, 0), -1)\n",
    "#         # ... (Các phần khác của code) ...\n",
    "# \n",
    "# # Tính toán tỷ lệ resize\n",
    "#         scale_x = w / 224  # w là chiều rộng của bounding box\n",
    "#         scale_y = h / 224  # h là chiều cao của bounding box\n",
    "#         \n",
    "#         # Chuyển đổi tọa độ landmark về ảnh gốc\n",
    "#         for i in range(68):\n",
    "#             landmark_x = int(predictions[0, i, 0] * scale_x + x)  # x là tọa độ x của bounding box\n",
    "#             landmark_y = int(predictions[0, i, 1] * scale_y + y)  # y là tọa độ y của bounding box\n",
    "#         \n",
    "#             # Vẽ landmark lên ảnh gốc\n",
    "#             cv2.circle(frame, (landmark_x, landmark_y), 2, (0, 255, 0), -1)\n",
    "# \n",
    "#         \n",
    "#         # Vẽ filter (ví dụ: vẽ mắt kính)\n",
    "#         left_eye_center = (int(predictions[0, 36, 0] + x), int(predictions[0, 36, 1] + y))\n",
    "#         right_eye_center = (int(predictions[0, 45, 0] + x), int(predictions[0, 45, 1] + y))\n",
    "#         eye_distance = int(np.linalg.norm(np.array(left_eye_center) - np.array(right_eye_center)))\n",
    "#         distance_factor = 1 + (eye_distance / frame.shape[1])  # Tỷ lệ khoảng cách mắt với chiều rộng khung hình\n",
    "#         glasses_width = int(eye_distance * 1.5 * distance_factor)  # Tăng hệ số nhân\n",
    "#         glasses_height = int(glasses_width * 0.4)\n",
    "#         glasses_x = int(left_eye_center[0] - glasses_width * 0.25)\n",
    "#         glasses_y = int(left_eye_center[1] - glasses_height * 0.5)\n",
    "# \n",
    "#         # cv2.rectangle(frame, (glasses_x, glasses_y), (glasses_x + glasses_width, glasses_y + glasses_height), (0, 0, 255), 2)\n",
    "# \n",
    "#     # Hiển thị khung hình\n",
    "#     cv2.imshow('Realtime Filter', frame)\n",
    "#     # Thoát khi nhấn phím 'q'\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# \n",
    "# # Giải phóng camera và đóng cửa sổ\n",
    "# cap.release()\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# ... (Các hàm và lớp đã được định nghĩa trước đó, bao gồm ResNet50, Transforms, ...) ...\n",
    "\n",
    "# Thiết bị (GPU nếu có)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Nạp mô hình đã huấn luyện\n",
    "best_network = ResNet50()  # Hoặc kiến trúc mạng tương ứng\n",
    "best_network.load_state_dict(torch.load('checkpoint/trainRS50_120.pth'))\n",
    "best_network.to(device)\n",
    "best_network.eval()\n",
    "\n",
    "# Nạp mô hình phát hiện khuôn mặt Haar Cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_and_draw_landmarks_realtime():\n",
    "    \"\"\"\n",
    "    Phát hiện khuôn mặt và vẽ landmark trong thời gian thực từ camera.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while(True):\n",
    "        # Đọc một khung hình từ camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Phát hiện khuôn mặt\n",
    "        faces = face_cascade.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=8)\n",
    "\n",
    "        # Tiền xử lý ảnh và dự đoán landmark cho từng khuôn mặt\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_image_gray = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n",
    "            face_image = Image.fromarray(face_image_gray)\n",
    "            face_image = transforms.Resize((224, 224))(face_image)\n",
    "            face_image = transforms.ToTensor()(face_image)\n",
    "            face_image = transforms.Normalize([0.5], [0.5])(face_image)\n",
    "            face_image = face_image.unsqueeze(0).to(device)\n",
    "\n",
    "            # Dự đoán landmark\n",
    "            with torch.no_grad():\n",
    "                predictions = best_network(face_image)\n",
    "            predictions = (predictions.cpu() + 0.5) * 224\n",
    "            predictions = predictions.view(-1, 68, 2).numpy()\n",
    "\n",
    "            # Tính toán tỷ lệ resize\n",
    "            scale_x = w / 224\n",
    "            scale_y = h / 224\n",
    "            \n",
    "            # Chuyển đổi tọa độ landmark về ảnh gốc\n",
    "            for i in range(68):\n",
    "                landmark_x = int(predictions[0, i, 0] * scale_x + x)\n",
    "                landmark_y = int(predictions[0, i, 1] * scale_y + y)\n",
    "            \n",
    "                # Vẽ landmark lên ảnh gốc\n",
    "                cv2.circle(frame, (landmark_x, landmark_y), 2, (0, 255, 0), -1)\n",
    "\n",
    "        # Hiển thị khung hình\n",
    "        cv2.imshow('Realtime Filter', frame)\n",
    "        # Thoát khi nhấn phím 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Giải phóng camera và đóng cửa sổ\n",
    "    cap.release()\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def detect_and_draw_landmarks_image(image_path):\n",
    "    \"\"\"\n",
    "    Phát hiện khuôn mặt và vẽ landmark từ một ảnh.\n",
    "    \"\"\"\n",
    "    # Đọc ảnh mới\n",
    "    image = cv2.imread(image_path, 1)\n",
    "\n",
    "    # Phát hiện khuôn mặt\n",
    "    faces = face_cascade.detectMultiScale(image,1.1, 4)\n",
    "\n",
    "    # Tiền xử lý ảnh\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_image_gray = cv2.cvtColor(image[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n",
    "        face_image = Image.fromarray(face_image_gray)\n",
    "        face_image = transforms.Resize((224, 224))(face_image)\n",
    "        face_image = transforms.ToTensor()(face_image)\n",
    "        face_image = transforms.Normalize([0.5], [0.5])(face_image)\n",
    "        face_image = face_image.unsqueeze(0).to(device)\n",
    "\n",
    "        # Dự đoán landmark\n",
    "        with torch.no_grad():\n",
    "            predictions = best_network(face_image)\n",
    "        predictions = (predictions.cpu() + 0.5) * 224\n",
    "        predictions = predictions.view(-1, 68, 2).numpy()\n",
    "        \n",
    "        scale_x = w / 224\n",
    "        scale_y = h / 224\n",
    "            \n",
    "        # Chuyển đổi tọa độ landmark về ảnh gốc\n",
    "        for i in range(68):\n",
    "            landmark_x = int(predictions[0, i, 0] * scale_x + x)\n",
    "            landmark_y = int(predictions[0, i, 1] * scale_y + y)\n",
    "        \n",
    "            # Vẽ landmark lên ảnh gốc\n",
    "            cv2.circle(image, (landmark_x, landmark_y), 2, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)    \n",
    "\n",
    "    current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Tạo thư mục results nếu chưa tồn tại\n",
    "    results_dir = 'Results'\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # Lưu kết quả với tên file là ngày giờ\n",
    "    output_filename = f\"{current_datetime}.jpg\"\n",
    "    output_path = os.path.join(results_dir, output_filename)\n",
    "    cv2.imwrite(output_path, image)\n",
    "    # Hiển thị kết quả\n",
    "    cv2.imshow('Result', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Gọi hàm để chạy\n",
    "# detect_and_draw_landmarks_realtime()  # Chạy với camera\n",
    "detect_and_draw_landmarks_image('test3.jpg')  # Chạy với ảnh"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
